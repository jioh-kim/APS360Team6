{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVNJ4GqZekQL"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-datasets librosa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import librosa\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tsBGHT_meq7T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nsynth_parser(data_split,\n",
        "                  batch_size = 64,\n",
        "                  num_batches = None,\n",
        "                  instrument_label = 4,\n",
        "                  source_type = None):\n",
        "  # Constants\n",
        "  INSTRUMENT_LABEL = instrument_label # According to NSynth dataset family label\n",
        "  SOURCE_TYPE = source_type\n",
        "  SAMPLE_RATE = 16000\n",
        "  TRIM_LENGTH = 3 * SAMPLE_RATE  # Trim to the first 3 seconds\n",
        "\n",
        "  def process_data(example):\n",
        "      audio = example['audio']\n",
        "      instrument_family = example['instrument']['family']\n",
        "      instrument_source = example['instrument']['source']\n",
        "      pitch = example['pitch']\n",
        "\n",
        "      is_keyboard = tf.equal(instrument_family, INSTRUMENT_LABEL)\n",
        "      is_source = tf.equal(True,True)\n",
        "      if SOURCE_TYPE is not None:\n",
        "          is_source = tf.equal(instrument_source, SOURCE_TYPE)\n",
        "\n",
        "      check_valid = tf.equal(is_keyboard,is_source)\n",
        "\n",
        "      def process_instrument_sample(audio, pitch):\n",
        "          audio = audio[:TRIM_LENGTH]\n",
        "          if pitch < 21:\n",
        "              # Instead of returning None, return a marker (e.g., a zero-length tensor)\n",
        "              return tf.zeros((0,)), tf.constant(-1, dtype=tf.int64)\n",
        "          else:\n",
        "              pitch = pitch - 21\n",
        "              return audio, pitch\n",
        "\n",
        "      cond_out = tf.cond(check_valid, lambda: process_instrument_sample(audio, pitch), lambda: (audio, pitch))\n",
        "      return cond_out\n",
        "\n",
        "  def filter_instrument_samples(example):\n",
        "      return tf.equal(example['instrument']['family'], INSTRUMENT_LABEL)\n",
        "\n",
        "  def filter_source_samples(example):\n",
        "      return tf.equal(example['instrument']['source'], SOURCE_TYPE)\n",
        "\n",
        "  def filter_invalid_samples(audio, pitch):\n",
        "      # Check if the sample is valid (not marked for removal)\n",
        "      return tf.size(audio) > 0 and tf.not_equal(pitch, -1)\n",
        "\n",
        "  def get_data_loader(data_split, batch_size=64, num_batches=None):\n",
        "      ds = tfds.load('nsynth', split=data_split, as_supervised=False)\n",
        "      ds = ds.filter(filter_instrument_samples)\n",
        "      if SOURCE_TYPE is not None:\n",
        "          ds = ds.filter(filter_source_samples)\n",
        "      ds = ds.map(process_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      ds = ds.filter(filter_invalid_samples)\n",
        "      ds = ds.batch(batch_size)\n",
        "      if num_batches:\n",
        "          ds = ds.take(num_batches)\n",
        "      ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "      return ds\n",
        "\n",
        "  ds = get_data_loader(data_split,batch_size,num_batches)\n",
        "  return ds\n"
      ],
      "metadata": {
        "id": "dsrZpg0Ees2p"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specifying instrument and data processing parameters\n",
        "batch_size = 32\n",
        "instrument_label = 4 #keyboard (default so doesn't have to be passed through)\n",
        "# generating the loaders\n",
        "train_loader = nsynth_parser('train', batch_size, num_batches=80)\n",
        "val_loader = nsynth_parser('valid', batch_size, num_batches=10)\n",
        "test_loader = nsynth_parser('test', batch_size, num_batches=10)\n",
        "classes = list(range(88))"
      ],
      "metadata": {
        "id": "9yLpQXMGfARu"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing a sample audio and pitch value\n",
        "for audio, pitch in train_loader.take(1):\n",
        "    first_sample_audio = audio[0]\n",
        "    first_sample_pitch = pitch[0]\n",
        "\n",
        "    # Calculate the maximum and minimum values\n",
        "    max_value = tf.reduce_max(first_sample_audio)\n",
        "    min_value = tf.reduce_min(first_sample_audio)\n",
        "\n",
        "    # Calculate the range (max - min)\n",
        "    range_value = max_value - min_value\n",
        "\n",
        "    print(f\"First sample audio max value: {max_value.numpy()}\")\n",
        "    print(f\"First sample audio min value: {min_value.numpy()}\")\n",
        "    print(f\"First sample audio range: {range_value.numpy()}\")\n",
        "    print(f\"First sample pitch: {first_sample_pitch.numpy()}\")"
      ],
      "metadata": {
        "id": "6l7JoYxt11oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of batches in each loader\n",
        "def get_dataset_length(data_loader):\n",
        "    length = 0\n",
        "    for _ in data_loader:\n",
        "        length += 1\n",
        "    return length\n",
        "\n",
        "# Use this function to get the length of your data loaders\n",
        "test_loader_length = get_dataset_length(test_loader)\n",
        "val_loader_length = get_dataset_length(val_loader)\n",
        "train_loader_length = get_dataset_length(train_loader)\n",
        "\n",
        "print(f\"Train loader length: {train_loader_length}\")\n",
        "print(f\"Validation loader length: {val_loader_length}\")\n",
        "print(f\"Test loader length: {test_loader_length}\")"
      ],
      "metadata": {
        "id": "ZIIXEBz52JG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of samples in each loader\n",
        "def get_dataset_sample_count(data_loader):\n",
        "    total_samples = 0\n",
        "    for audio, pitch in data_loader:\n",
        "        # Count the number of samples in each batch\n",
        "        batch_samples = tf.shape(audio)[0]  # assuming audio is a 2D tensor [batch_size, features]\n",
        "        total_samples += batch_samples\n",
        "    return total_samples\n",
        "\n",
        "# Use this function to get the number of samples in your data loaders\n",
        "test_samples_count = get_dataset_sample_count(test_loader)\n",
        "val_samples_count = get_dataset_sample_count(val_loader)\n",
        "train_samples_count = get_dataset_sample_count(train_loader)\n",
        "\n",
        "print(f\"Train loader samples: {train_samples_count}\")\n",
        "print(f\"Validation loader samples: {val_samples_count}\")\n",
        "print(f\"Test loader samples: {test_samples_count}\")"
      ],
      "metadata": {
        "id": "ikrw7lLX2h2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################################################\n",
        "####################################################################### Model and Training #####################################################################\n",
        "################################################################################################################################################################"
      ],
      "metadata": {
        "id": "m-WkcWF92rwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers, models"
      ],
      "metadata": {
        "id": "fjHa3y3F2tKZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PitchDetectionModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(PitchDetectionModel, self).__init__()\n",
        "        self.reshape = layers.Reshape((48000, 1))\n",
        "        self.conv1 = layers.Conv1D(1024, kernel_size=4, strides=4, activation='relu')\n",
        "        self.drop1 = layers.Dropout(0.2)  # Dropout layer after conv1\n",
        "        self.conv2 = layers.Conv1D(128, kernel_size=4, strides=4, activation='relu')\n",
        "        self.drop2 = layers.Dropout(0.2)  # Dropout layer after conv4\n",
        "        self.conv3 = layers.Conv1D(128, kernel_size=4, strides=4, activation='relu')\n",
        "        self.drop3 = layers.Dropout(0.5)  # Dropout layer after conv4\n",
        "        self.conv4 = layers.Conv1D(256, kernel_size=2, strides=2, activation='relu')\n",
        "        self.drop4 = layers.Dropout(0.5)  # Dropout layer after conv4\n",
        "        self.pool = layers.MaxPooling1D(2)\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.fc1 = layers.Dense(88)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.reshape(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.drop1(x, training=training)  # Apply dropout only during training\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.drop2(x, training=training)\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.drop3(x, training=training)\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.drop4(x, training=training)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc1(x)\n"
      ],
      "metadata": {
        "id": "kZnJeT8N22dC"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the model\n",
        "model = PitchDetectionModel()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(0.001),  # Optimizer\n",
        "    # Loss function to minimize\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    # List of metrics to monitor\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "print(\"Fit model on training data\")\n",
        "history = model.fit(\n",
        "  x=train_loader,\n",
        "  epochs=35,\n",
        "  verbose=1,\n",
        "  validation_data=val_loader\n",
        ")"
      ],
      "metadata": {
        "id": "azK4tL_t25jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2yG4JJmR3ITe"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_accuracy = history.history['sparse_categorical_accuracy']\n",
        "validation_accuracy = history.history['val_sparse_categorical_accuracy']\n",
        "\n",
        "epochs = range(1, len(training_accuracy) + 1)\n",
        "\n",
        "# Plotting the accuracy graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, training_accuracy, label='Training Accuracy')\n",
        "plt.plot(epochs, validation_accuracy, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yef-EP3T3F0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_loader)\n",
        "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')"
      ],
      "metadata": {
        "id": "tlKNRwkR8v8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################################################\n",
        "####################################################################### Evaluation on New Data #################################################################\n",
        "################################################################################################################################################################"
      ],
      "metadata": {
        "id": "tfxcCPL786iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specifying instrument and data processing parameters\n",
        "batch_size = 32\n",
        "instrument_label = 6 #organ\n",
        "source_type = 1 #electronic\n",
        "# generating the loader\n",
        "organ_test_loader = nsynth_parser('test',\n",
        "                                  batch_size,\n",
        "                                  num_batches=10,\n",
        "                                  instrument_label=instrument_label,\n",
        "                                  source_type=source_type)"
      ],
      "metadata": {
        "id": "KVRyo_N79F-s"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate organ_test_loader\n",
        "test_loss, test_acc = model.evaluate(organ_test_loader)\n",
        "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')"
      ],
      "metadata": {
        "id": "nmWKPlJ3-EAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################################################\n",
        "####################################################################### Baseline Model #########################################################################\n",
        "################################################################################################################################################################"
      ],
      "metadata": {
        "id": "ghmZ2CZg-N3F"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 16000\n",
        "def pYIN_pitch_estimate_accuracy(audio,true_pitch):\n",
        "  f0, voiced_flag, _ = librosa.pyin(audio, fmin=librosa.note_to_hz('C1'), fmax=librosa.note_to_hz('C8'), sr=SAMPLE_RATE)\n",
        "  estimated_midi_notes = librosa.hz_to_midi(f0)\n",
        "  # MIDI notes can be fractional, so we use a small tolerance to consider two pitches to be equal\n",
        "  tolerance = 0.5\n",
        "  correct_estimations = np.sum(np.abs(estimated_midi_notes - true_pitch) <= tolerance)\n",
        "  total_voiced_frames = np.sum(~np.isnan(estimated_midi_notes))\n",
        "  # Calculate the accuracy\n",
        "  accuracy = correct_estimations / total_voiced_frames if total_voiced_frames > 0 else 0\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "fugbtq_3Atua"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_as_np_iterator = organ_test_loader.unbatch().as_numpy_iterator()\n",
        "accuracy_accross_samples = list()\n",
        "for audio, pitch in test_dataset_as_np_iterator:\n",
        "  accuracy_accross_samples.append(pYIN_pitch_estimate_accuracy(audio,pitch + 21))\n",
        "average_accuracy = np.mean(accuracy_accross_samples)\n",
        "print(f'Average accuracy: {average_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0-ruC3u-ROl",
        "outputId": "c15a7755-1da5-47cd-cef9-86f02baafc86"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy: 0.8179077547919589\n"
          ]
        }
      ]
    }
  ]
}