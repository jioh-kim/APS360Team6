{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THz8nEmmRcAf"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfEwnLErRel8"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku_YwLwCqS6w"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-datasets librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZAVKWS5R1_J"
      },
      "source": [
        "Only grab the dataset where instument == keyboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10KY4gUceq2a"
      },
      "outputs": [],
      "source": [
        "# =================== Augmentation Function Library ==================\n",
        "def noise_injection(audio, noise_factor = 0.001):\n",
        "  '''Inject random/white noise into the audio'''\n",
        "  noise = np.random.randn(len(audio))\n",
        "  return audio + noise_factor * noise\n",
        "\n",
        "def change_speed(audio, speed_factor = 2):\n",
        "  '''\n",
        "  Changes the speed of playback by scaling time\n",
        "  speed_factor > 1      => speeds up playback\n",
        "  0 < speed_factor < 1  => slows down playback\n",
        "  '''\n",
        "  return librosa.effects.time_stretch(audio, rate=speed_factor)\n",
        "\n",
        "def change_speed_and_fit(audio, stretch_factor = 0.5):\n",
        "  '''Changes speed similar to change_speed and fits into the same length as the input audio'''\n",
        "  input_length = len(audio)\n",
        "  stretched_audio = librosa.effects.time_stretch(audio.astype('float'), rate=stretch_factor)\n",
        "  if len(stretched_audio) > input_length:\n",
        "    return stretched_audio[:input_length]\n",
        "  else:\n",
        "    return np.pad(stretched_audio, (0, max(0, input_length - len(stretched_audio))), \"constant\")\n",
        "\n",
        "def shift_pitch(audio, steps = 10, sr=16000):\n",
        "  '''Shifts the pitch up by the specified amount of steps'''\n",
        "  return librosa.effects.pitch_shift(audio, sr=sr, n_steps=steps)\n",
        "\n",
        "def shift_time(audio, shift_factor = 0.5, random = False):\n",
        "  '''Shifts the audio in time by a given factor, keeping the same length of the intial audio'''\n",
        "  if random:\n",
        "    shift_factor = shift_factor * 2 * (np.random.uniform() - 0.5)\n",
        "\n",
        "  start = int(len(audio) * shift_factor)\n",
        "  if (start > 0):\n",
        "      return np.pad(audio,(start,0),mode='constant')[0:len(audio)]\n",
        "  else:\n",
        "      return np.pad(audio,(0,-start),mode='constant')[0:len(audio)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vcosBrKmKQev"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Constants\n",
        "KEYBOARD_FAMILY_LABEL = 4 # According to NSynth dataset family label\n",
        "SAMPLE_RATE = 16000\n",
        "TRIM_LENGTH = 3 * SAMPLE_RATE  # Trim to the first 3 seconds\n",
        "\n",
        "# Define the processing function\n",
        "def process_data(example):\n",
        "    audio = example['audio']\n",
        "    instrument_family = example['instrument']['family']\n",
        "    pitch = example['pitch']\n",
        "\n",
        "    # Filter keyboard samples\n",
        "    is_keyboard = tf.equal(instrument_family, KEYBOARD_FAMILY_LABEL)\n",
        "    # Only process the samples where is_keyboard is True\n",
        "    def process_keyboard_sample(audio, pitch):\n",
        "        # Trim the audio\n",
        "        audio = audio[:TRIM_LENGTH]\n",
        "\n",
        "        # Convert audio to CQT (Constant-Q Transform) using librosa\n",
        "        # Set fmin to the frequency of A0 and n_bins to 88\n",
        "        def compute_cqt(x):\n",
        "            return np.abs(librosa.cqt(x, sr=SAMPLE_RATE, fmin=librosa.note_to_hz('A0'), n_bins=88, bins_per_octave=12))\n",
        "\n",
        "        # Here, tf.numpy_function applies a Python function to the TensorFlow tensor\n",
        "        audio = tf.numpy_function(compute_cqt, [audio], tf.float32)\n",
        "\n",
        "        # Modify pitch\n",
        "        pitch = pitch - 21\n",
        "        return audio, pitch\n",
        "    # Return the processed audio and pitch, only if the sample is a keyboard\n",
        "    return tf.cond(is_keyboard, lambda: process_keyboard_sample(audio, pitch), lambda: (audio, pitch))\n",
        "\n",
        "def filter_keyboard_samples(example):\n",
        "    return tf.equal(example['instrument']['family'], KEYBOARD_FAMILY_LABEL)\n",
        "\n",
        "def get_data_loader(data_split, batch_size=64):\n",
        "    ds = tfds.load('nsynth', split=data_split, as_supervised=False)\n",
        "\n",
        "    # First, filter out non-keyboard samples\n",
        "    ds = ds.filter(filter_keyboard_samples)\n",
        "    ds = ds.map(process_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.filter(lambda audio, pitch: tf.reduce_sum(tf.shape(audio)) > 0)  # Filter out empty audio results\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Input to model\n",
        "batch_size = 64\n",
        "test_loader = get_data_loader('test', batch_size)\n",
        "val_loader = get_data_loader('valid', batch_size)\n",
        "train_loader = get_data_loader('train', batch_size)\n",
        "classes = list(range(88))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IGad4g33Vwh",
        "outputId": "35ea7de7-4e17-4b27-b706-9a1a3929e7a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio shape: (64, 88, 94)\n",
            "Pitch shape: (64,)\n",
            "Audio shape: (64, 88, 94)\n",
            "Pitch shape: (64,)\n",
            "Audio shape: (64, 88, 94)\n",
            "Pitch shape: (64,)\n"
          ]
        }
      ],
      "source": [
        "for audio, pitch in test_loader.take(1):\n",
        "    print(\"Audio shape:\", audio.shape)\n",
        "    print(\"Pitch shape:\", pitch.shape)\n",
        "\n",
        "for audio, pitch in val_loader.take(1):\n",
        "    print(\"Audio shape:\", audio.shape)\n",
        "    print(\"Pitch shape:\", pitch.shape)\n",
        "\n",
        "for audio, pitch in train_loader.take(1):\n",
        "    print(\"Audio shape:\", audio.shape)\n",
        "    print(\"Pitch shape:\", pitch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "W67Mcd6USB_n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "from google.colab import files\n",
        "import dask.dataframe as dd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvCczG5tUQtI",
        "outputId": "f022bc2b-5039-4509-819c-b1d8c0913f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Batch 0 Loss: 7.962560176849365 Accuracy: 0.00%\n",
            "Batch 1 Loss: 7.894560813903809 Accuracy: 0.00%\n",
            "Batch 2 Loss: 7.7659831047058105 Accuracy: 0.52%\n",
            "Batch 3 Loss: 7.764236927032471 Accuracy: 0.78%\n",
            "Batch 4 Loss: 7.685639381408691 Accuracy: 0.62%\n",
            "Batch 5 Loss: 7.570768356323242 Accuracy: 0.52%\n",
            "Batch 6 Loss: 7.541556358337402 Accuracy: 0.45%\n",
            "Batch 7 Loss: 7.313226699829102 Accuracy: 0.59%\n",
            "Batch 8 Loss: 7.520594120025635 Accuracy: 0.87%\n",
            "Batch 9 Loss: 7.407275676727295 Accuracy: 0.94%\n",
            "Batch 10 Loss: 7.186352252960205 Accuracy: 0.85%\n",
            "Batch 11 Loss: 7.287949562072754 Accuracy: 0.91%\n",
            "Epoch: 1\n",
            "Batch 0 Loss: 7.445340156555176 Accuracy: 0.00%\n",
            "Batch 1 Loss: 7.228724002838135 Accuracy: 0.00%\n",
            "Batch 2 Loss: 7.021732807159424 Accuracy: 0.00%\n",
            "Batch 3 Loss: 7.178116798400879 Accuracy: 0.39%\n",
            "Batch 4 Loss: 7.101716995239258 Accuracy: 0.31%\n",
            "Batch 5 Loss: 7.016138076782227 Accuracy: 0.26%\n",
            "Batch 6 Loss: 6.990074157714844 Accuracy: 0.22%\n",
            "Batch 7 Loss: 6.760902404785156 Accuracy: 0.39%\n",
            "Batch 8 Loss: 7.175092697143555 Accuracy: 0.69%\n",
            "Batch 9 Loss: 6.967657089233398 Accuracy: 0.62%\n",
            "Batch 10 Loss: 6.71852970123291 Accuracy: 0.57%\n",
            "Batch 11 Loss: 6.960908889770508 Accuracy: 0.52%\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "class PitchDetectionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PitchDetectionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=88, out_channels=32, kernel_size=5)  # Adjusted in_channels to 88\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=5)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=5)\n",
        "        # self.conv4 = nn.Conv1d(in_channels=16, out_channels=64, kernel_size=1)\n",
        "        # self.conv5 = nn.Conv1d(in_channels=64, out_channels=88, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()  # Convert x to float32 datatype\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "\n",
        "        # x = F.relu(self.conv2(x))\n",
        "        # x = F.relu(self.conv3(x))\n",
        "        # x = F.relu(self.conv4(x))\n",
        "        # x = F.relu(self.conv5(x))\n",
        "        # x = x.permute(0, 2, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = PitchDetectionModel()\n",
        "\n",
        "\n",
        "def train_net(net, data_loader, num_epochs):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch:', epoch)\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(data_loader):\n",
        "            inputs = torch.tensor(inputs.numpy(), device=device).float()\n",
        "            labels = torch.tensor(labels.numpy(), device=device).long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Get the predictions from the maximum value\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Total number of labels\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "            # Total correct predictions\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracy = correct_predictions / total_predictions\n",
        "\n",
        "            print(f\"Batch {i} Loss: {loss.item()} Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        # epoch_loss = running_loss / len(data_loader)\n",
        "        # epoch_accuracy = correct_predictions / total_predictions\n",
        "        # print(f\"Epoch {epoch + 1} completed. Loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy * 100:.2f}%\")\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "train_net(model, test_loader, 5)\n",
        "\n",
        "\n",
        "train_net(model, test_loader, 2)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
