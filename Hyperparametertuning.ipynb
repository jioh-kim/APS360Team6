{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jioh-kim/ML-Audio-to-Pitch/blob/main/Hyperparametertuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THz8nEmmRcAf"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku_YwLwCqS6w"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-datasets librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfEwnLErRel8"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import librosa\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WGHxbh4YE-ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZAVKWS5R1_J"
      },
      "source": [
        "Only grab the dataset where instument == keyboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "KEYBOARD_FAMILY_LABEL = 4 # According to NSynth dataset family label\n",
        "SAMPLE_RATE = 16000\n",
        "TRIM_LENGTH = 3 * SAMPLE_RATE  # Trim to the first 3 seconds\n",
        "\n",
        "def process_data(example):\n",
        "    audio = example['audio']\n",
        "    instrument_family = example['instrument']['family']\n",
        "    pitch = example['pitch']\n",
        "\n",
        "    is_keyboard = tf.equal(instrument_family, KEYBOARD_FAMILY_LABEL)\n",
        "\n",
        "    def process_keyboard_sample(audio, pitch):\n",
        "        audio = audio[:TRIM_LENGTH]\n",
        "        if pitch < 21:\n",
        "            # Instead of returning None, return a marker (e.g., a zero-length tensor)\n",
        "            return tf.zeros((0,)), tf.constant(-1, dtype=tf.int64)\n",
        "        else:\n",
        "            pitch = pitch - 21\n",
        "            return audio, pitch\n",
        "\n",
        "    return tf.cond(is_keyboard, lambda: process_keyboard_sample(audio, pitch), lambda: (audio, pitch))\n",
        "\n",
        "def filter_keyboard_samples(example):\n",
        "    return tf.equal(example['instrument']['family'], KEYBOARD_FAMILY_LABEL)\n",
        "\n",
        "def filter_invalid_samples(audio, pitch):\n",
        "    # Check if the sample is valid (not marked for removal)\n",
        "    return tf.size(audio) > 0 and tf.not_equal(pitch, -1)\n",
        "\n",
        "def get_data_loader(data_split, batch_size=64, num_batches=None):\n",
        "    ds = tfds.load('nsynth', split=data_split, as_supervised=False)\n",
        "\n",
        "    ds = ds.filter(filter_keyboard_samples)\n",
        "    ds = ds.map(process_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.filter(filter_invalid_samples)\n",
        "    ds = ds.batch(batch_size)\n",
        "    if num_batches:\n",
        "        ds = ds.take(num_batches)\n",
        "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds"
      ],
      "metadata": {
        "id": "RKOifdhZeOT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input to model\n",
        "batch_size = 8\n",
        "train_loader = get_data_loader('train', batch_size, num_batches=40)\n",
        "val_loader = get_data_loader('valid', batch_size, num_batches=6)\n",
        "test_loader = get_data_loader('test', batch_size, num_batches=6)\n",
        "classes = list(range(88))\n",
        "\n",
        "for audio, pitch in train_loader.take(5):\n",
        "    print(pitch)"
      ],
      "metadata": {
        "id": "8qV0GWm8gC0r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "e8a406d9cb4c46f68b9d51948affb53e",
            "8c1061bd780f426491a368f95f8b6eef",
            "35510a99126f446eabf828880e5ac950",
            "0cd8809c6a624770afa02094d49ac38f",
            "f0b539daf0dd42d2aa172bf1f716b0d4",
            "2054e108a7d7452290e49f747e0226e8",
            "3283b1e592084513b63848a9608056a1",
            "756f3f0122f44d1195bdbbcc22de777a",
            "99ef5c50ccec41f19a217403fb794a58",
            "9d50832b04d4482b87493a9184c8b633",
            "943e626a55a54496bcff0947affd1402"
          ]
        },
        "outputId": "0b792628-c9d3-441f-b0d3-d9d89152dc03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.\n",
            "WARNING:absl:You use TensorFlow DType <dtype: 'bool'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to bool.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 73.07 GiB (download: 73.07 GiB, generated: 73.09 GiB, total: 146.16 GiB) to /root/tensorflow_datasets/nsynth/full/2.3.3...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...:   0%|          | 0/1069 [00:00<?, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8a406d9cb4c46f68b9d51948affb53e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset nsynth downloaded and prepared to /root/tensorflow_datasets/nsynth/full/2.3.3. Subsequent calls will reuse this data.\n",
            "tf.Tensor([85 10 60 33 48 84 21 54], shape=(8,), dtype=int64)\n",
            "tf.Tensor([13 33  0  6 49 52 56 17], shape=(8,), dtype=int64)\n",
            "tf.Tensor([ 4 74  6 40 63 20 44 41], shape=(8,), dtype=int64)\n",
            "tf.Tensor([ 7 22 67 64  8 78 16 48], shape=(8,), dtype=int64)\n",
            "tf.Tensor([14 29 87 32 70 84 65 66], shape=(8,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IGad4g33Vwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01909091-5c31-4ab3-9da0-00e554cce496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sample audio max value: 0.46275418996810913\n",
            "First sample audio min value: -0.46886491775512695\n",
            "First sample audio range: 0.9316191077232361\n",
            "First sample pitch: 85\n"
          ]
        }
      ],
      "source": [
        "# printing a sample audio and pitch value\n",
        "for audio, pitch in train_loader.take(1):\n",
        "    first_sample_audio = audio[0]\n",
        "    first_sample_pitch = pitch[0]\n",
        "\n",
        "    # Calculate the maximum and minimum values\n",
        "    max_value = tf.reduce_max(first_sample_audio)\n",
        "    min_value = tf.reduce_min(first_sample_audio)\n",
        "\n",
        "    # Calculate the range (max - min)\n",
        "    range_value = max_value - min_value\n",
        "\n",
        "    print(f\"First sample audio max value: {max_value.numpy()}\")\n",
        "    print(f\"First sample audio min value: {min_value.numpy()}\")\n",
        "    print(f\"First sample audio range: {range_value.numpy()}\")\n",
        "    print(f\"First sample pitch: {first_sample_pitch.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for audio, pitch in train_loader:\n",
        "  print(audio.shape)\n",
        "  print(pitch.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLg7K5F4Mw6R",
        "outputId": "e4417755-b5d2-497a-e07b-be771d9d2559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n",
            "(64, 48000)\n",
            "(64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of batches in each loader\n",
        "def get_dataset_length(data_loader):\n",
        "    length = 0\n",
        "    for _ in data_loader:\n",
        "        length += 1\n",
        "    return length\n",
        "\n",
        "# Use this function to get the length of your data loaders\n",
        "test_loader_length = get_dataset_length(test_loader)\n",
        "val_loader_length = get_dataset_length(val_loader)\n",
        "train_loader_length = get_dataset_length(train_loader)\n",
        "\n",
        "print(f\"Train loader length: {train_loader_length}\")\n",
        "print(f\"Validation loader length: {val_loader_length}\")\n",
        "print(f\"Test loader length: {test_loader_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtGkBcScNX6O",
        "outputId": "9aa5f6f9-a4b0-4b2e-a52a-adbd07bb2744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader length: 40\n",
            "Validation loader length: 6\n",
            "Test loader length: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of samples in each loader\n",
        "def get_dataset_sample_count(data_loader):\n",
        "    total_samples = 0\n",
        "    for audio, pitch in data_loader:\n",
        "        # Count the number of samples in each batch\n",
        "        batch_samples = tf.shape(audio)[0]  # assuming audio is a 2D tensor [batch_size, features]\n",
        "        total_samples += batch_samples\n",
        "    return total_samples\n",
        "\n",
        "# Use this function to get the number of samples in your data loaders\n",
        "test_samples_count = get_dataset_sample_count(test_loader)\n",
        "val_samples_count = get_dataset_sample_count(val_loader)\n",
        "train_samples_count = get_dataset_sample_count(train_loader)\n",
        "\n",
        "print(f\"Train loader samples: {train_samples_count}\")\n",
        "print(f\"Validation loader samples: {val_samples_count}\")\n",
        "print(f\"Test loader samples: {test_samples_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOqZSyzqZOqp",
        "outputId": "4f2fa067-b93a-4238-8c6c-93d83ad29bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader samples: 320\n",
            "Validation loader samples: 48\n",
            "Test loader samples: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################################################################################################\n",
        "####################################################################### Model and Training #####################################################################\n",
        "################################################################################################################################################################"
      ],
      "metadata": {
        "id": "NugixcBbTB0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(10)"
      ],
      "metadata": {
        "id": "K93FP75yTTES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f45078-2118-4c54-ddf9-556f4802b97a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a53d5551dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for audio, pitch in train_loader.take(1):\n",
        "  print(f\"Audio shape: {audio.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWXjA-_1Vn1x",
        "outputId": "24444f3f-d30d-46e2-e7ef-d4a6906626b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio shape: (64, 48000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architecture\n",
        "\n",
        "'''\n",
        "class PitchDetectionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PitchDetectionModel, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=1,     out_channels=1024,  kernel_size=4, stride=4)\n",
        "    self.conv2 = nn.Conv1d(in_channels=1024,  out_channels=128,   kernel_size=4, stride=4)\n",
        "    self.conv3 = nn.Conv1d(in_channels=128,   out_channels=128,   kernel_size=4, stride=4)\n",
        "    self.conv4 = nn.Conv1d(in_channels=128,   out_channels=256,   kernel_size=2, stride=2)\n",
        "    self.pool  = nn.MaxPool1d(2, 2)\n",
        "    self.fc1   = nn.Linear(256*23, 88)\n",
        "\n",
        "  def forward(self, x):                   # Input length 48,000,  channels 1\n",
        "    x = self.pool(F.relu(self.conv1(x)))  #       length 6,000,   channels 1024\n",
        "    x = self.pool(F.relu(self.conv2(x)))  #       length 750,     channels 128\n",
        "    x = self.pool(F.relu(self.conv3(x)))  #       length 93,      channels 128\n",
        "    x = self.pool(F.relu(self.conv4(x)))  #       length 23,      channels 256\n",
        "    x = x.view(-1, 256*23)                # Flatten the tensor\n",
        "    x = self.fc1(x)                       # Output length 88\n",
        "    # x = F.softmax(self.fc1(x))\n",
        "    return x                              # Assuming CELoss\n",
        "'''"
      ],
      "metadata": {
        "id": "UuXgpGqKTO9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers, models"
      ],
      "metadata": {
        "id": "o5NaUzFYmPvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class PitchDetectionModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(PitchDetectionModel, self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv1D(in_channels=1,     out_channels=1024,  kernel_size=4, stride=4)\n",
        "    self.conv2 = tf.keras.layers.Conv1D(in_channels=1024,  out_channels=128,   kernel_size=4, stride=4)\n",
        "    self.conv3 = tf.keras.layers.Conv1D(in_channels=128,   out_channels=128,   kernel_size=4, stride=4)\n",
        "    self.conv4 = tf.keras.layers.Conv1D(in_channels=128,   out_channels=256,   kernel_size=2, stride=2)\n",
        "    self.pool  = tf.keras.layers.MaxPool1D(2, 2)\n",
        "    self.fc1   = tf.keras.layers.Dense(88)\n",
        "\n",
        "  def call(self, x):                   # Input length 48,000,  channels 1\n",
        "    x = self.pool(tf.nn.relu(self.conv1(x)))  #       length 6,000,   channels 1024\n",
        "    x = self.pool(tf.nn.relu(self.conv2(x)))  #       length 750,     channels 128\n",
        "    x = self.pool(tf.nn.relu(self.conv3(x)))  #       length 93,      channels 128\n",
        "    x = self.pool(tf.nn.relu(self.conv4(x)))  #       length 23,      channels 256\n",
        "    x = tf.reshape(x, (-1, 256*23))                # Flatten the tensor\n",
        "    x = self.fc1(x)                       # Output length 88\n",
        "    # x = tf.nn.softmax(self.fc1(x))\n",
        "    return x                              # Assuming CELoss\n",
        "\n",
        "model = PitchDetectionModel()\n",
        "'''"
      ],
      "metadata": {
        "id": "3VAR6qnOQdbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PitchDetectionModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(PitchDetectionModel, self).__init__()\n",
        "        self.reshape = layers.Reshape((48000, 1))\n",
        "        self.conv1 = layers.Conv1D(1024, kernel_size=4, strides=4, activation='relu')\n",
        "        self.conv2 = layers.Conv1D(128, kernel_size=4, strides=4, activation='relu')\n",
        "        self.conv3 = layers.Conv1D(128, kernel_size=4, strides=4, activation='relu')\n",
        "        self.conv4 = layers.Conv1D(256, kernel_size=2, strides=2, activation='relu')\n",
        "        self.pool = layers.MaxPooling1D(2)\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.fc1 = layers.Dense(88)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.reshape(x)\n",
        "        x = self.pool(self.conv1(x))\n",
        "        x = self.pool(self.conv2(x))\n",
        "        x = self.pool(self.conv3(x))\n",
        "        x = self.pool(self.conv4(x))\n",
        "        x = self.flatten(x)\n",
        "        return self.fc1(x)\n"
      ],
      "metadata": {
        "id": "VSAFmJQjmQkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class PitchDetectionModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(PitchDetectionModel, self).__init__()\n",
        "    self.reshape = layers.Reshape((48000, 1))\n",
        "    self.conv1 = layers.Conv1D(1024, kernel_size=4, strides=4, activation='relu')\n",
        "    self.conv2 = layers.Conv1D(128, kernel_size=4, strides=4, activation='relu')\n",
        "    self.conv3 = layers.Conv1D(128, kernel_size=4, strides=4, activation='relu')\n",
        "    self.conv4 = layers.Conv1D(256, kernel_size=2, strides=2, activation='relu')\n",
        "    self.pool = layers.MaxPooling1D(2)\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.fc1 = layers.Dense(88)\n",
        "\n",
        "  def call(self, x):                   # Input length 48,000,  channels 1\n",
        "    x = self.pool(tf.nn.relu(self.conv1(x)))  #       length 6,000,   channels 1024\n",
        "    x = self.pool(tf.nn.relu(self.conv2(x)))  #       length 750,     channels 128\n",
        "    x = self.pool(tf.nn.relu(self.conv3(x)))  #       length 93,      channels 128\n",
        "    x = self.pool(tf.nn.relu(self.conv4(x)))  #       length 23,      channels 256\n",
        "    x = tf.reshape(x, (-1, 256*23))                # Flatten the tensor\n",
        "    x = self.fc1(x)                       # Output length 88\n",
        "    # x = tf.nn.softmax(self.fc1(x))\n",
        "    return x                              # Assuming CELoss\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "BdaqtYg1SaO5",
        "outputId": "baad2aed-c207-442b-a2bb-6290f3e33abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nclass PitchDetectionModel(tf.keras.Model):\\n  def __init__(self):\\n    super(PitchDetectionModel, self).__init__()\\n    self.reshape = layers.Reshape((48000, 1))  \\n    self.conv1 = layers.Conv1D(1024, kernel_size=4, strides=4, activation='relu')\\n    self.conv2 = layers.Conv1D(128, kernel_size=4, strides=4, activation='relu')\\n    self.conv3 = layers.Conv1D(128, kernel_size=4, strides=4, activation='relu')\\n    self.conv4 = layers.Conv1D(256, kernel_size=2, strides=2, activation='relu')\\n    self.pool = layers.MaxPooling1D(2)\\n    self.flatten = layers.Flatten()\\n    self.fc1 = layers.Dense(88)\\n\\n  def call(self, x):                   # Input length 48,000,  channels 1\\n    x = self.pool(tf.nn.relu(self.conv1(x)))  #       length 6,000,   channels 1024\\n    x = self.pool(tf.nn.relu(self.conv2(x)))  #       length 750,     channels 128\\n    x = self.pool(tf.nn.relu(self.conv3(x)))  #       length 93,      channels 128\\n    x = self.pool(tf.nn.relu(self.conv4(x)))  #       length 23,      channels 256\\n    x = tf.reshape(x, (-1, 256*23))                # Flatten the tensor\\n    x = self.fc1(x)                       # Output length 88\\n    # x = tf.nn.softmax(self.fc1(x))\\n    return x                              # Assuming CELoss\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the model\n",
        "model = PitchDetectionModel()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),  # Optimizer\n",
        "    # Loss function to minimize\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    # List of metrics to monitor\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "# history = model.fit(train_loader, epochs=10, validation_data=val_loader)\n",
        "\n",
        "print(\"Fit model on training data\")\n",
        "history = model.fit(\n",
        "  x=train_loader,\n",
        "  epochs=10,\n",
        "  verbose=2,\n",
        "  validation_data=val_loader\n",
        ")\n",
        "'''\n",
        "print(\"Fit model on training data\")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=2,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    validation_data=(x_val, y_val),\n",
        ")\n",
        "'''\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_loader)\n",
        "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM4TAKEGLrh1",
        "outputId": "14e70694-916a-4d70-c1b1-3ff130c77509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fit model on training data\n",
            "Epoch 1/10\n",
            "40/40 - 15s - loss: 4.4835 - sparse_categorical_accuracy: 0.0125 - val_loss: 4.4686 - val_sparse_categorical_accuracy: 0.0000e+00 - 15s/epoch - 386ms/step\n",
            "Epoch 2/10\n",
            "40/40 - 3s - loss: 4.3076 - sparse_categorical_accuracy: 0.0250 - val_loss: 4.4129 - val_sparse_categorical_accuracy: 0.0000e+00 - 3s/epoch - 83ms/step\n",
            "Epoch 3/10\n",
            "40/40 - 3s - loss: 4.1243 - sparse_categorical_accuracy: 0.0594 - val_loss: 4.6557 - val_sparse_categorical_accuracy: 0.0208 - 3s/epoch - 83ms/step\n",
            "Epoch 4/10\n",
            "40/40 - 3s - loss: 3.8858 - sparse_categorical_accuracy: 0.0750 - val_loss: 5.0740 - val_sparse_categorical_accuracy: 0.0625 - 3s/epoch - 83ms/step\n",
            "Epoch 5/10\n",
            "40/40 - 3s - loss: 3.4776 - sparse_categorical_accuracy: 0.1312 - val_loss: 5.5558 - val_sparse_categorical_accuracy: 0.0417 - 3s/epoch - 83ms/step\n",
            "Epoch 6/10\n",
            "40/40 - 3s - loss: 2.9314 - sparse_categorical_accuracy: 0.2406 - val_loss: 6.2727 - val_sparse_categorical_accuracy: 0.1042 - 3s/epoch - 84ms/step\n",
            "Epoch 7/10\n",
            "40/40 - 3s - loss: 2.1878 - sparse_categorical_accuracy: 0.4156 - val_loss: 8.3224 - val_sparse_categorical_accuracy: 0.1250 - 3s/epoch - 84ms/step\n",
            "Epoch 8/10\n",
            "40/40 - 3s - loss: 1.5815 - sparse_categorical_accuracy: 0.5469 - val_loss: 11.1021 - val_sparse_categorical_accuracy: 0.1042 - 3s/epoch - 84ms/step\n",
            "Epoch 9/10\n",
            "40/40 - 3s - loss: 1.4061 - sparse_categorical_accuracy: 0.6313 - val_loss: 12.5350 - val_sparse_categorical_accuracy: 0.2083 - 3s/epoch - 82ms/step\n",
            "Epoch 10/10\n",
            "40/40 - 3s - loss: 1.1695 - sparse_categorical_accuracy: 0.7031 - val_loss: 13.0568 - val_sparse_categorical_accuracy: 0.2083 - 3s/epoch - 82ms/step\n",
            "6/6 [==============================] - 0s 34ms/step - loss: 7.5590 - sparse_categorical_accuracy: 0.0625\n",
            "Test accuracy: 0.0625, Test loss: 7.5590362548828125\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e8a406d9cb4c46f68b9d51948affb53e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c1061bd780f426491a368f95f8b6eef",
              "IPY_MODEL_35510a99126f446eabf828880e5ac950",
              "IPY_MODEL_0cd8809c6a624770afa02094d49ac38f"
            ],
            "layout": "IPY_MODEL_f0b539daf0dd42d2aa172bf1f716b0d4"
          }
        },
        "8c1061bd780f426491a368f95f8b6eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2054e108a7d7452290e49f747e0226e8",
            "placeholder": "​",
            "style": "IPY_MODEL_3283b1e592084513b63848a9608056a1",
            "value": "Dl Completed...: 100%"
          }
        },
        "35510a99126f446eabf828880e5ac950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_756f3f0122f44d1195bdbbcc22de777a",
            "max": 1069,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99ef5c50ccec41f19a217403fb794a58",
            "value": 1069
          }
        },
        "0cd8809c6a624770afa02094d49ac38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d50832b04d4482b87493a9184c8b633",
            "placeholder": "​",
            "style": "IPY_MODEL_943e626a55a54496bcff0947affd1402",
            "value": " 1069/1069 [06:00&lt;00:00,  2.47 file/s]"
          }
        },
        "f0b539daf0dd42d2aa172bf1f716b0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2054e108a7d7452290e49f747e0226e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3283b1e592084513b63848a9608056a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "756f3f0122f44d1195bdbbcc22de777a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ef5c50ccec41f19a217403fb794a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d50832b04d4482b87493a9184c8b633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943e626a55a54496bcff0947affd1402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}